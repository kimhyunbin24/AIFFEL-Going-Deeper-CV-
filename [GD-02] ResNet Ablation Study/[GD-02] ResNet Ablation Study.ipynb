{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2176667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675c403",
   "metadata": {},
   "source": [
    "## dataset, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a1e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "image_shape: (50000, 3, 32, 32)\n",
      "image_mean: [0.49139968, 0.48215827, 0.44653124]\n",
      "image_std: [0.24703233, 0.24348505, 0.26158768]\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "imgs = [img for img, _ in cifar_dataset]\n",
    "imgs = torch.stack(imgs, dim=0).numpy()\n",
    "\n",
    "cifar10_means = [\n",
    "    imgs[:, 0, :, :].mean(),\n",
    "    imgs[:, 1, :, :].mean(),\n",
    "    imgs[:, 2, :, :].mean(),\n",
    "]\n",
    "cifar10_stds = [\n",
    "    imgs[:, 0, :, :].std(),\n",
    "    imgs[:, 1, :, :].std(),\n",
    "    imgs[:, 2, :, :].std(),\n",
    "]\n",
    "\n",
    "print('image_shape:', imgs.shape)\n",
    "print('image_mean:', cifar10_means)\n",
    "print('image_std:', cifar10_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571c3069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.transforms import ColorJitter\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_means, std=cifar10_stds),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=img_transforms,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=img_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54df4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afbfea",
   "metadata": {},
   "source": [
    "## model block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d1a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        downsample=False,\n",
    "        dim_exp=False,\n",
    "    ):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.downsample_stride = 2 if downsample else 1\n",
    "        self.downsample = downsample\n",
    "        self.dim_exp = dim_exp\n",
    "\n",
    "        # Conv 3x3\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=self.downsample_stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Conv 3x3\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut Connection\n",
    "        if self.downsample:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=self.downsample_stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        if self.dim_exp:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # Final ReLU\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample or self.dim_exp:\n",
    "            identity = self.proj(identity)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        x += identity\n",
    "        out = self.relu2(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4749dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        mid_channels,\n",
    "        out_channels,\n",
    "        downsample=False,\n",
    "        dim_exp=False,\n",
    "    ):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.dim_exp = dim_exp\n",
    "        self.downsample_stride = 2 if self.downsample else 1\n",
    "\n",
    "        # Conv 1x1\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=mid_channels,\n",
    "            kernel_size=1,\n",
    "            stride=self.downsample_stride,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Conv 3x3\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=mid_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Conv 1x1\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=mid_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Shortcut Connection\n",
    "        if downsample:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=self.downsample_stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        elif dim_exp:\n",
    "            self.proj = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "        # Final ReLU\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample or self.dim_exp:\n",
    "            identity = self.proj(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # inplace Error -> Solution: using clone\n",
    "        # - Reference: https://github.com/NVlabs/FUNIT/issues/23\n",
    "        x = x.clone() + identity\n",
    "        out = self.relu4(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cf4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_layers=18, num_cls=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_cls = num_cls\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "        )\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        if num_layers == 18:\n",
    "            self.conv2 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=2,\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                downsample=False,\n",
    "                dim_exp=True,\n",
    "            )\n",
    "            self.conv3 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=2,\n",
    "                in_channels=128,\n",
    "                out_channels=256,\n",
    "                downsample=True,\n",
    "                dim_exp=True,\n",
    "            )\n",
    "            self.conv4 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=2,\n",
    "                in_channels=256,\n",
    "                out_channels=512,\n",
    "                downsample=True,\n",
    "                dim_exp=True,\n",
    "            )\n",
    "            self.conv5 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=2,\n",
    "                in_channels=512,\n",
    "                out_channels=1024,\n",
    "                downsample=True,\n",
    "                dim_exp=True,\n",
    "            )\n",
    "            self.fc_in_dim = 1024\n",
    "        else: # num_layers == 50\n",
    "            self.conv2 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=3,\n",
    "                in_channels=64,\n",
    "                mid_channels=64,\n",
    "                downsample = False,\n",
    "            )\n",
    "            self.conv3 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=4,\n",
    "                in_channels=64*4,\n",
    "                mid_channels=128,\n",
    "                downsample = True,\n",
    "            )\n",
    "            self.conv4 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=6,\n",
    "                in_channels=128*4,\n",
    "                mid_channels=256,\n",
    "                downsample = True,\n",
    "            )\n",
    "            self.conv5 = self._make_layers(\n",
    "                num_layers=num_layers,\n",
    "                num_stacks=3,\n",
    "                in_channels=256*4,\n",
    "                mid_channels=512,\n",
    "                downsample = True,\n",
    "            )\n",
    "            self.fc_in_dim = 512*4\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=1, stride=1)\n",
    "        # self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_layer = nn.Linear(self.fc_in_dim, self.num_cls)\n",
    "\n",
    "        # Weight Initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layers(\n",
    "        self,\n",
    "        num_layers,\n",
    "        num_stacks,\n",
    "        in_channels=0,\n",
    "        mid_channels=0,\n",
    "        out_channels=0,\n",
    "        downsample=False,\n",
    "        dim_exp=False,\n",
    "    ):\n",
    "        layers = []\n",
    "        if num_layers == 18:\n",
    "            layers.append(\n",
    "                ResidualBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    downsample=downsample,\n",
    "                ),\n",
    "            )\n",
    "            layers.append(\n",
    "                ResidualBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dim_exp=dim_exp,\n",
    "                ),\n",
    "            )\n",
    "        else: # num_layers == 50\n",
    "            layers.append(\n",
    "                Bottleneck(\n",
    "                    in_channels=in_channels,\n",
    "                    mid_channels=mid_channels,\n",
    "                    out_channels=mid_channels*4,\n",
    "                    downsample=downsample,\n",
    "                    dim_exp=True,\n",
    "                )\n",
    "            )\n",
    "            for i in range(1, num_stacks):\n",
    "                if i == (num_stacks-1):\n",
    "                    print('last')\n",
    "                    dim_exp=True\n",
    "\n",
    "                layers.append(\n",
    "                    Bottleneck(\n",
    "                        in_channels=mid_channels*4,\n",
    "                        mid_channels=mid_channels,\n",
    "                        out_channels=mid_channels*4,\n",
    "                        downsample=False,\n",
    "                        dim_exp=dim_exp,\n",
    "                    )\n",
    "                )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.avg_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        out = self.fc_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9707c",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8dba8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last\n",
      "last\n",
      "last\n",
      "last\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu2): ReLU(inplace=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu3): ReLU(inplace=True)\n",
      "      (proj): Sequential(\n",
      "        (0): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu4): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc_layer): Linear(in_features=2048, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ResNet 50\n",
    "\n",
    "num_layers = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ResNet(num_layers=num_layers).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0026c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=1e-1,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=epochs, # epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a8cca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, scheduler, device):\n",
    "    total_nums = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        preds = model(images)\n",
    "        loss = loss_fn(preds, labels)\n",
    "        losses.append(loss.item() * len(images))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, cur_nums = loss.item(), batch * len(images)\n",
    "            print(f'loss: {loss:>7f} [{cur_nums:>5d}/{total_nums:>5d}]')\n",
    "\n",
    "    scheduler.step()\n",
    "    return sum(losses)/total_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc5d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device):\n",
    "    total_nums = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "\n",
    "    loss, corrects = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            preds = model(images)\n",
    "            loss += loss_fn(preds, labels).item() * len(images)\n",
    "            corrects += (preds.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "        loss /= total_nums\n",
    "        acc = corrects / total_nums * 100\n",
    "        print(f'Test Results:\\n Accuracy: {acc:>0.1f}%, Avg Loss: {loss:>8f}\\n')\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13a7aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "------------------------------\n",
      "loss: 2.354125 [    0/50000]\n",
      "loss: 2.325865 [10000/50000]\n",
      "loss: 3.277433 [20000/50000]\n",
      "loss: 2.573221 [30000/50000]\n",
      "loss: 2.432005 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 19.6%, Avg Loss: 2.633646\n",
      "\n",
      "Epoch 2:\n",
      "------------------------------\n",
      "loss: 2.067981 [    0/50000]\n",
      "loss: 1.953318 [10000/50000]\n",
      "loss: 2.027076 [20000/50000]\n",
      "loss: 1.900735 [30000/50000]\n",
      "loss: 2.101641 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 23.0%, Avg Loss: 2.014706\n",
      "\n",
      "Epoch 3:\n",
      "------------------------------\n",
      "loss: 1.943487 [    0/50000]\n",
      "loss: 2.188975 [10000/50000]\n",
      "loss: 1.947597 [20000/50000]\n",
      "loss: 1.853439 [30000/50000]\n",
      "loss: 1.996073 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 23.4%, Avg Loss: 2.013268\n",
      "\n",
      "Epoch 4:\n",
      "------------------------------\n",
      "loss: 1.814456 [    0/50000]\n",
      "loss: 2.037652 [10000/50000]\n",
      "loss: 1.830893 [20000/50000]\n",
      "loss: 1.792281 [30000/50000]\n",
      "loss: 1.679724 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 28.3%, Avg Loss: 2.129116\n",
      "\n",
      "Epoch 5:\n",
      "------------------------------\n",
      "loss: 1.843475 [    0/50000]\n",
      "loss: 1.970202 [10000/50000]\n",
      "loss: 1.780064 [20000/50000]\n",
      "loss: 1.646310 [30000/50000]\n",
      "loss: 1.781072 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 29.6%, Avg Loss: 1.868069\n",
      "\n",
      "Epoch 6:\n",
      "------------------------------\n",
      "loss: 1.790203 [    0/50000]\n",
      "loss: 1.736245 [10000/50000]\n",
      "loss: 1.692461 [20000/50000]\n",
      "loss: 1.682861 [30000/50000]\n",
      "loss: 1.763416 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 32.5%, Avg Loss: 1.808767\n",
      "\n",
      "Epoch 7:\n",
      "------------------------------\n",
      "loss: 1.752816 [    0/50000]\n",
      "loss: 1.691918 [10000/50000]\n",
      "loss: 1.640643 [20000/50000]\n",
      "loss: 1.681216 [30000/50000]\n",
      "loss: 1.808747 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 37.2%, Avg Loss: 1.751833\n",
      "\n",
      "Epoch 8:\n",
      "------------------------------\n",
      "loss: 1.761892 [    0/50000]\n",
      "loss: 1.650738 [10000/50000]\n",
      "loss: 1.702149 [20000/50000]\n",
      "loss: 1.544972 [30000/50000]\n",
      "loss: 1.519764 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 37.7%, Avg Loss: 1.733188\n",
      "\n",
      "Epoch 9:\n",
      "------------------------------\n",
      "loss: 1.702460 [    0/50000]\n",
      "loss: 1.738541 [10000/50000]\n",
      "loss: 1.561605 [20000/50000]\n",
      "loss: 1.456970 [30000/50000]\n",
      "loss: 1.544179 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 39.5%, Avg Loss: 1.658917\n",
      "\n",
      "Epoch 10:\n",
      "------------------------------\n",
      "loss: 1.343006 [    0/50000]\n",
      "loss: 1.507460 [10000/50000]\n",
      "loss: 1.436989 [20000/50000]\n",
      "loss: 1.482160 [30000/50000]\n",
      "loss: 1.686270 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 40.9%, Avg Loss: 1.650396\n",
      "\n",
      "Epoch 11:\n",
      "------------------------------\n",
      "loss: 1.552404 [    0/50000]\n",
      "loss: 1.672154 [10000/50000]\n",
      "loss: 1.783329 [20000/50000]\n",
      "loss: 1.467863 [30000/50000]\n",
      "loss: 1.380750 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 44.6%, Avg Loss: 1.697902\n",
      "\n",
      "Epoch 12:\n",
      "------------------------------\n",
      "loss: 1.414715 [    0/50000]\n",
      "loss: 1.547469 [10000/50000]\n",
      "loss: 1.467885 [20000/50000]\n",
      "loss: 1.509406 [30000/50000]\n",
      "loss: 1.479742 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 44.2%, Avg Loss: 1.665910\n",
      "\n",
      "Epoch 13:\n",
      "------------------------------\n",
      "loss: 1.490390 [    0/50000]\n",
      "loss: 1.518210 [10000/50000]\n",
      "loss: 1.499417 [20000/50000]\n",
      "loss: 1.251038 [30000/50000]\n",
      "loss: 1.611889 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 49.0%, Avg Loss: 1.747405\n",
      "\n",
      "Epoch 14:\n",
      "------------------------------\n",
      "loss: 1.339188 [    0/50000]\n",
      "loss: 1.703068 [10000/50000]\n",
      "loss: 1.422881 [20000/50000]\n",
      "loss: 1.465935 [30000/50000]\n",
      "loss: 1.561722 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 49.4%, Avg Loss: 1.621917\n",
      "\n",
      "Epoch 15:\n",
      "------------------------------\n",
      "loss: 1.162028 [    0/50000]\n",
      "loss: 1.337520 [10000/50000]\n",
      "loss: 1.349179 [20000/50000]\n",
      "loss: 1.257352 [30000/50000]\n",
      "loss: 1.297074 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 52.4%, Avg Loss: 1.397707\n",
      "\n",
      "Epoch 16:\n",
      "------------------------------\n",
      "loss: 1.343534 [    0/50000]\n",
      "loss: 1.494549 [10000/50000]\n",
      "loss: 1.184657 [20000/50000]\n",
      "loss: 1.277050 [30000/50000]\n",
      "loss: 1.104122 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 52.9%, Avg Loss: 1.320322\n",
      "\n",
      "Epoch 17:\n",
      "------------------------------\n",
      "loss: 1.123930 [    0/50000]\n",
      "loss: 1.259342 [10000/50000]\n",
      "loss: 1.252596 [20000/50000]\n",
      "loss: 1.352747 [30000/50000]\n",
      "loss: 1.129273 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 55.2%, Avg Loss: 2.401749\n",
      "\n",
      "Epoch 18:\n",
      "------------------------------\n",
      "loss: 1.177863 [    0/50000]\n",
      "loss: 1.367884 [10000/50000]\n",
      "loss: 1.337622 [20000/50000]\n",
      "loss: 1.053400 [30000/50000]\n",
      "loss: 1.191266 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 56.4%, Avg Loss: 1.255278\n",
      "\n",
      "Epoch 19:\n",
      "------------------------------\n",
      "loss: 1.134687 [    0/50000]\n",
      "loss: 1.047938 [10000/50000]\n",
      "loss: 1.293231 [20000/50000]\n",
      "loss: 1.258837 [30000/50000]\n",
      "loss: 1.093938 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 55.1%, Avg Loss: 1.292970\n",
      "\n",
      "Epoch 20:\n",
      "------------------------------\n",
      "loss: 1.269496 [    0/50000]\n",
      "loss: 1.137855 [10000/50000]\n",
      "loss: 1.451339 [20000/50000]\n",
      "loss: 1.296716 [30000/50000]\n",
      "loss: 1.243436 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 56.4%, Avg Loss: 1.267901\n",
      "\n",
      "Epoch 21:\n",
      "------------------------------\n",
      "loss: 1.381244 [    0/50000]\n",
      "loss: 1.177719 [10000/50000]\n",
      "loss: 1.190111 [20000/50000]\n",
      "loss: 1.183809 [30000/50000]\n",
      "loss: 0.940067 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 57.5%, Avg Loss: 1.207015\n",
      "\n",
      "Epoch 22:\n",
      "------------------------------\n",
      "loss: 1.247982 [    0/50000]\n",
      "loss: 1.135352 [10000/50000]\n",
      "loss: 1.076247 [20000/50000]\n",
      "loss: 1.177014 [30000/50000]\n",
      "loss: 1.278036 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 60.1%, Avg Loss: 1.125028\n",
      "\n",
      "Epoch 23:\n",
      "------------------------------\n",
      "loss: 1.064013 [    0/50000]\n",
      "loss: 1.157287 [10000/50000]\n",
      "loss: 1.147540 [20000/50000]\n",
      "loss: 1.159694 [30000/50000]\n",
      "loss: 1.154249 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 59.9%, Avg Loss: 1.126970\n",
      "\n",
      "Epoch 24:\n",
      "------------------------------\n",
      "loss: 1.164896 [    0/50000]\n",
      "loss: 1.070527 [10000/50000]\n",
      "loss: 1.045589 [20000/50000]\n",
      "loss: 0.991763 [30000/50000]\n",
      "loss: 1.073427 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 63.2%, Avg Loss: 1.057802\n",
      "\n",
      "Epoch 25:\n",
      "------------------------------\n",
      "loss: 0.906538 [    0/50000]\n",
      "loss: 0.989118 [10000/50000]\n",
      "loss: 1.173633 [20000/50000]\n",
      "loss: 1.074296 [30000/50000]\n",
      "loss: 0.965571 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 63.3%, Avg Loss: 1.047217\n",
      "\n",
      "Epoch 26:\n",
      "------------------------------\n",
      "loss: 0.887728 [    0/50000]\n",
      "loss: 0.957952 [10000/50000]\n",
      "loss: 0.942564 [20000/50000]\n",
      "loss: 0.848407 [30000/50000]\n",
      "loss: 0.838484 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 63.7%, Avg Loss: 1.024824\n",
      "\n",
      "Epoch 27:\n",
      "------------------------------\n",
      "loss: 1.028846 [    0/50000]\n",
      "loss: 1.009016 [10000/50000]\n",
      "loss: 0.914334 [20000/50000]\n",
      "loss: 0.933830 [30000/50000]\n",
      "loss: 0.935475 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 66.7%, Avg Loss: 0.963502\n",
      "\n",
      "Epoch 28:\n",
      "------------------------------\n",
      "loss: 0.830953 [    0/50000]\n",
      "loss: 0.792880 [10000/50000]\n",
      "loss: 0.916869 [20000/50000]\n",
      "loss: 0.979064 [30000/50000]\n",
      "loss: 0.852157 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 66.5%, Avg Loss: 0.937242\n",
      "\n",
      "Epoch 29:\n",
      "------------------------------\n",
      "loss: 0.800891 [    0/50000]\n",
      "loss: 0.891923 [10000/50000]\n",
      "loss: 0.828265 [20000/50000]\n",
      "loss: 0.761061 [30000/50000]\n",
      "loss: 0.917534 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 68.0%, Avg Loss: 0.940810\n",
      "\n",
      "Epoch 30:\n",
      "------------------------------\n",
      "loss: 0.899794 [    0/50000]\n",
      "loss: 0.918411 [10000/50000]\n",
      "loss: 0.916048 [20000/50000]\n",
      "loss: 0.877480 [30000/50000]\n",
      "loss: 0.751273 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 70.1%, Avg Loss: 0.872250\n",
      "\n",
      "Epoch 31:\n",
      "------------------------------\n",
      "loss: 0.812031 [    0/50000]\n",
      "loss: 0.990758 [10000/50000]\n",
      "loss: 0.976606 [20000/50000]\n",
      "loss: 0.620467 [30000/50000]\n",
      "loss: 0.859834 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 70.7%, Avg Loss: 0.855260\n",
      "\n",
      "Epoch 32:\n",
      "------------------------------\n",
      "loss: 0.885170 [    0/50000]\n",
      "loss: 0.597211 [10000/50000]\n",
      "loss: 0.702606 [20000/50000]\n",
      "loss: 0.655982 [30000/50000]\n",
      "loss: 0.613929 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 70.0%, Avg Loss: 0.865678\n",
      "\n",
      "Epoch 33:\n",
      "------------------------------\n",
      "loss: 0.752954 [    0/50000]\n",
      "loss: 0.751257 [10000/50000]\n",
      "loss: 0.945863 [20000/50000]\n",
      "loss: 1.074387 [30000/50000]\n",
      "loss: 0.695016 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 72.0%, Avg Loss: 0.819859\n",
      "\n",
      "Epoch 34:\n",
      "------------------------------\n",
      "loss: 0.582034 [    0/50000]\n",
      "loss: 0.712622 [10000/50000]\n",
      "loss: 0.689593 [20000/50000]\n",
      "loss: 0.800031 [30000/50000]\n",
      "loss: 0.631155 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 72.5%, Avg Loss: 0.809824\n",
      "\n",
      "Epoch 35:\n",
      "------------------------------\n",
      "loss: 0.828166 [    0/50000]\n",
      "loss: 0.819881 [10000/50000]\n",
      "loss: 0.665363 [20000/50000]\n",
      "loss: 0.709721 [30000/50000]\n",
      "loss: 0.808792 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 72.4%, Avg Loss: 0.827318\n",
      "\n",
      "Epoch 36:\n",
      "------------------------------\n",
      "loss: 0.654637 [    0/50000]\n",
      "loss: 0.868849 [10000/50000]\n",
      "loss: 0.600555 [20000/50000]\n",
      "loss: 0.546536 [30000/50000]\n",
      "loss: 0.769808 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 72.8%, Avg Loss: 0.797299\n",
      "\n",
      "Epoch 37:\n",
      "------------------------------\n",
      "loss: 0.621846 [    0/50000]\n",
      "loss: 0.971193 [10000/50000]\n",
      "loss: 0.833521 [20000/50000]\n",
      "loss: 0.838427 [30000/50000]\n",
      "loss: 0.544136 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 74.4%, Avg Loss: 0.768446\n",
      "\n",
      "Epoch 38:\n",
      "------------------------------\n",
      "loss: 0.625213 [    0/50000]\n",
      "loss: 0.845781 [10000/50000]\n",
      "loss: 0.590207 [20000/50000]\n",
      "loss: 0.505906 [30000/50000]\n",
      "loss: 0.705713 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 75.1%, Avg Loss: 0.736204\n",
      "\n",
      "Epoch 39:\n",
      "------------------------------\n",
      "loss: 0.651601 [    0/50000]\n",
      "loss: 0.747748 [10000/50000]\n",
      "loss: 0.667744 [20000/50000]\n",
      "loss: 0.613918 [30000/50000]\n",
      "loss: 0.781938 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 73.8%, Avg Loss: 0.768050\n",
      "\n",
      "Epoch 40:\n",
      "------------------------------\n",
      "loss: 0.770726 [    0/50000]\n",
      "loss: 0.637357 [10000/50000]\n",
      "loss: 0.679766 [20000/50000]\n",
      "loss: 0.615182 [30000/50000]\n",
      "loss: 0.636627 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 74.1%, Avg Loss: 0.764914\n",
      "\n",
      "Epoch 41:\n",
      "------------------------------\n",
      "loss: 0.733502 [    0/50000]\n",
      "loss: 0.575810 [10000/50000]\n",
      "loss: 0.702457 [20000/50000]\n",
      "loss: 0.577638 [30000/50000]\n",
      "loss: 0.629704 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 75.1%, Avg Loss: 0.725464\n",
      "\n",
      "Epoch 42:\n",
      "------------------------------\n",
      "loss: 0.640130 [    0/50000]\n",
      "loss: 0.572437 [10000/50000]\n",
      "loss: 0.518544 [20000/50000]\n",
      "loss: 0.578659 [30000/50000]\n",
      "loss: 0.850081 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 74.2%, Avg Loss: 0.783917\n",
      "\n",
      "Epoch 43:\n",
      "------------------------------\n",
      "loss: 0.673030 [    0/50000]\n",
      "loss: 0.589706 [10000/50000]\n",
      "loss: 0.831947 [20000/50000]\n",
      "loss: 0.681915 [30000/50000]\n",
      "loss: 0.639857 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 75.5%, Avg Loss: 0.710288\n",
      "\n",
      "Epoch 44:\n",
      "------------------------------\n",
      "loss: 0.620279 [    0/50000]\n",
      "loss: 0.572248 [10000/50000]\n",
      "loss: 0.361548 [20000/50000]\n",
      "loss: 0.655846 [30000/50000]\n",
      "loss: 0.598755 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 76.0%, Avg Loss: 0.705251\n",
      "\n",
      "Epoch 45:\n",
      "------------------------------\n",
      "loss: 0.625973 [    0/50000]\n",
      "loss: 0.587058 [10000/50000]\n",
      "loss: 0.659321 [20000/50000]\n",
      "loss: 0.580684 [30000/50000]\n",
      "loss: 0.470742 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 76.6%, Avg Loss: 0.690457\n",
      "\n",
      "Epoch 46:\n",
      "------------------------------\n",
      "loss: 0.589695 [    0/50000]\n",
      "loss: 0.557106 [10000/50000]\n",
      "loss: 0.664242 [20000/50000]\n",
      "loss: 0.481912 [30000/50000]\n",
      "loss: 0.510996 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 77.9%, Avg Loss: 0.654299\n",
      "\n",
      "Epoch 47:\n",
      "------------------------------\n",
      "loss: 0.480469 [    0/50000]\n",
      "loss: 0.664452 [10000/50000]\n",
      "loss: 0.468215 [20000/50000]\n",
      "loss: 0.481579 [30000/50000]\n",
      "loss: 0.539252 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 75.5%, Avg Loss: 0.715325\n",
      "\n",
      "Epoch 48:\n",
      "------------------------------\n",
      "loss: 0.594898 [    0/50000]\n",
      "loss: 0.532352 [10000/50000]\n",
      "loss: 0.547890 [20000/50000]\n",
      "loss: 0.668362 [30000/50000]\n",
      "loss: 0.319486 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 77.2%, Avg Loss: 0.686139\n",
      "\n",
      "Epoch 49:\n",
      "------------------------------\n",
      "loss: 0.584690 [    0/50000]\n",
      "loss: 0.553513 [10000/50000]\n",
      "loss: 0.579297 [20000/50000]\n",
      "loss: 0.540755 [30000/50000]\n",
      "loss: 0.656350 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 77.3%, Avg Loss: 0.669689\n",
      "\n",
      "Epoch 50:\n",
      "------------------------------\n",
      "loss: 0.468558 [    0/50000]\n",
      "loss: 0.638660 [10000/50000]\n",
      "loss: 0.397821 [20000/50000]\n",
      "loss: 0.521784 [30000/50000]\n",
      "loss: 0.475624 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.6%, Avg Loss: 0.633809\n",
      "\n",
      "Epoch 51:\n",
      "------------------------------\n",
      "loss: 0.596349 [    0/50000]\n",
      "loss: 0.750213 [10000/50000]\n",
      "loss: 0.404960 [20000/50000]\n",
      "loss: 0.576595 [30000/50000]\n",
      "loss: 0.521561 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.0%, Avg Loss: 0.651124\n",
      "\n",
      "Epoch 52:\n",
      "------------------------------\n",
      "loss: 0.377133 [    0/50000]\n",
      "loss: 0.454549 [10000/50000]\n",
      "loss: 0.494560 [20000/50000]\n",
      "loss: 0.579427 [30000/50000]\n",
      "loss: 0.521973 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.4%, Avg Loss: 0.640338\n",
      "\n",
      "Epoch 53:\n",
      "------------------------------\n",
      "loss: 0.452311 [    0/50000]\n",
      "loss: 0.421918 [10000/50000]\n",
      "loss: 0.496061 [20000/50000]\n",
      "loss: 0.408909 [30000/50000]\n",
      "loss: 0.322069 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 79.2%, Avg Loss: 0.611035\n",
      "\n",
      "Epoch 54:\n",
      "------------------------------\n",
      "loss: 0.585672 [    0/50000]\n",
      "loss: 0.483042 [10000/50000]\n",
      "loss: 0.479831 [20000/50000]\n",
      "loss: 0.527032 [30000/50000]\n",
      "loss: 0.495069 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.4%, Avg Loss: 0.647210\n",
      "\n",
      "Epoch 55:\n",
      "------------------------------\n",
      "loss: 0.587706 [    0/50000]\n",
      "loss: 0.551216 [10000/50000]\n",
      "loss: 0.534430 [20000/50000]\n",
      "loss: 0.578343 [30000/50000]\n",
      "loss: 0.458448 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 79.7%, Avg Loss: 0.600493\n",
      "\n",
      "Epoch 56:\n",
      "------------------------------\n",
      "loss: 0.461479 [    0/50000]\n",
      "loss: 0.468151 [10000/50000]\n",
      "loss: 0.409261 [20000/50000]\n",
      "loss: 0.527220 [30000/50000]\n",
      "loss: 0.474225 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.5%, Avg Loss: 0.652451\n",
      "\n",
      "Epoch 57:\n",
      "------------------------------\n",
      "loss: 0.452917 [    0/50000]\n",
      "loss: 0.437383 [10000/50000]\n",
      "loss: 0.449525 [20000/50000]\n",
      "loss: 0.234726 [30000/50000]\n",
      "loss: 0.281364 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.7%, Avg Loss: 0.642721\n",
      "\n",
      "Epoch 58:\n",
      "------------------------------\n",
      "loss: 0.396229 [    0/50000]\n",
      "loss: 0.615132 [10000/50000]\n",
      "loss: 0.512157 [20000/50000]\n",
      "loss: 0.565176 [30000/50000]\n",
      "loss: 0.305977 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 79.4%, Avg Loss: 0.625294\n",
      "\n",
      "Epoch 59:\n",
      "------------------------------\n",
      "loss: 0.433202 [    0/50000]\n",
      "loss: 0.409829 [10000/50000]\n",
      "loss: 0.551455 [20000/50000]\n",
      "loss: 0.474143 [30000/50000]\n",
      "loss: 0.454702 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.3%, Avg Loss: 0.657208\n",
      "\n",
      "Epoch 60:\n",
      "------------------------------\n",
      "loss: 0.352717 [    0/50000]\n",
      "loss: 0.405232 [10000/50000]\n",
      "loss: 0.418404 [20000/50000]\n",
      "loss: 0.388295 [30000/50000]\n",
      "loss: 0.443086 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 78.7%, Avg Loss: 0.627196\n",
      "\n",
      "Epoch 61:\n",
      "------------------------------\n",
      "loss: 0.411147 [    0/50000]\n",
      "loss: 0.507416 [10000/50000]\n",
      "loss: 0.649179 [20000/50000]\n",
      "loss: 0.606988 [30000/50000]\n",
      "loss: 0.349036 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 80.7%, Avg Loss: 0.590305\n",
      "\n",
      "Epoch 62:\n",
      "------------------------------\n",
      "loss: 0.395423 [    0/50000]\n",
      "loss: 0.478058 [10000/50000]\n",
      "loss: 0.375730 [20000/50000]\n",
      "loss: 0.455438 [30000/50000]\n",
      "loss: 0.457291 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.3%, Avg Loss: 0.560471\n",
      "\n",
      "Epoch 63:\n",
      "------------------------------\n",
      "loss: 0.413233 [    0/50000]\n",
      "loss: 0.381877 [10000/50000]\n",
      "loss: 0.321189 [20000/50000]\n",
      "loss: 0.332091 [30000/50000]\n",
      "loss: 0.361334 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.0%, Avg Loss: 0.567056\n",
      "\n",
      "Epoch 64:\n",
      "------------------------------\n",
      "loss: 0.284143 [    0/50000]\n",
      "loss: 0.344517 [10000/50000]\n",
      "loss: 0.345034 [20000/50000]\n",
      "loss: 0.419544 [30000/50000]\n",
      "loss: 0.356692 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 80.5%, Avg Loss: 0.587422\n",
      "\n",
      "Epoch 65:\n",
      "------------------------------\n",
      "loss: 0.389532 [    0/50000]\n",
      "loss: 0.383238 [10000/50000]\n",
      "loss: 0.341359 [20000/50000]\n",
      "loss: 0.284800 [30000/50000]\n",
      "loss: 0.654614 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.5%, Avg Loss: 0.569718\n",
      "\n",
      "Epoch 66:\n",
      "------------------------------\n",
      "loss: 0.334291 [    0/50000]\n",
      "loss: 0.244031 [10000/50000]\n",
      "loss: 0.392423 [20000/50000]\n",
      "loss: 0.405979 [30000/50000]\n",
      "loss: 0.301959 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 80.4%, Avg Loss: 0.604673\n",
      "\n",
      "Epoch 67:\n",
      "------------------------------\n",
      "loss: 0.390986 [    0/50000]\n",
      "loss: 0.289061 [10000/50000]\n",
      "loss: 0.371234 [20000/50000]\n",
      "loss: 0.347204 [30000/50000]\n",
      "loss: 0.401890 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.5%, Avg Loss: 0.576443\n",
      "\n",
      "Epoch 68:\n",
      "------------------------------\n",
      "loss: 0.344056 [    0/50000]\n",
      "loss: 0.298218 [10000/50000]\n",
      "loss: 0.440906 [20000/50000]\n",
      "loss: 0.254854 [30000/50000]\n",
      "loss: 0.391163 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.6%, Avg Loss: 0.565363\n",
      "\n",
      "Epoch 69:\n",
      "------------------------------\n",
      "loss: 0.279119 [    0/50000]\n",
      "loss: 0.223041 [10000/50000]\n",
      "loss: 0.239383 [20000/50000]\n",
      "loss: 0.289719 [30000/50000]\n",
      "loss: 0.294315 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.5%, Avg Loss: 0.580043\n",
      "\n",
      "Epoch 70:\n",
      "------------------------------\n",
      "loss: 0.305291 [    0/50000]\n",
      "loss: 0.245685 [10000/50000]\n",
      "loss: 0.355133 [20000/50000]\n",
      "loss: 0.544851 [30000/50000]\n",
      "loss: 0.374938 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 81.8%, Avg Loss: 0.570684\n",
      "\n",
      "Epoch 71:\n",
      "------------------------------\n",
      "loss: 0.269636 [    0/50000]\n",
      "loss: 0.239731 [10000/50000]\n",
      "loss: 0.409369 [20000/50000]\n",
      "loss: 0.417795 [30000/50000]\n",
      "loss: 0.286001 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.1%, Avg Loss: 0.555522\n",
      "\n",
      "Epoch 72:\n",
      "------------------------------\n",
      "loss: 0.257435 [    0/50000]\n",
      "loss: 0.144090 [10000/50000]\n",
      "loss: 0.243436 [20000/50000]\n",
      "loss: 0.313826 [30000/50000]\n",
      "loss: 0.440043 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.3%, Avg Loss: 0.554273\n",
      "\n",
      "Epoch 73:\n",
      "------------------------------\n",
      "loss: 0.219894 [    0/50000]\n",
      "loss: 0.313281 [10000/50000]\n",
      "loss: 0.393470 [20000/50000]\n",
      "loss: 0.375732 [30000/50000]\n",
      "loss: 0.253582 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.4%, Avg Loss: 0.566840\n",
      "\n",
      "Epoch 74:\n",
      "------------------------------\n",
      "loss: 0.218310 [    0/50000]\n",
      "loss: 0.198841 [10000/50000]\n",
      "loss: 0.276794 [20000/50000]\n",
      "loss: 0.299568 [30000/50000]\n",
      "loss: 0.307626 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.3%, Avg Loss: 0.565079\n",
      "\n",
      "Epoch 75:\n",
      "------------------------------\n",
      "loss: 0.330978 [    0/50000]\n",
      "loss: 0.270206 [10000/50000]\n",
      "loss: 0.216483 [20000/50000]\n",
      "loss: 0.192520 [30000/50000]\n",
      "loss: 0.398646 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.3%, Avg Loss: 0.564912\n",
      "\n",
      "Epoch 76:\n",
      "------------------------------\n",
      "loss: 0.281468 [    0/50000]\n",
      "loss: 0.231735 [10000/50000]\n",
      "loss: 0.198046 [20000/50000]\n",
      "loss: 0.242941 [30000/50000]\n",
      "loss: 0.235958 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.7%, Avg Loss: 0.588798\n",
      "\n",
      "Epoch 77:\n",
      "------------------------------\n",
      "loss: 0.244040 [    0/50000]\n",
      "loss: 0.283058 [10000/50000]\n",
      "loss: 0.294789 [20000/50000]\n",
      "loss: 0.173850 [30000/50000]\n",
      "loss: 0.294547 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.0%, Avg Loss: 0.559513\n",
      "\n",
      "Epoch 78:\n",
      "------------------------------\n",
      "loss: 0.208396 [    0/50000]\n",
      "loss: 0.203975 [10000/50000]\n",
      "loss: 0.248654 [20000/50000]\n",
      "loss: 0.371525 [30000/50000]\n",
      "loss: 0.199108 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 82.8%, Avg Loss: 0.576796\n",
      "\n",
      "Epoch 79:\n",
      "------------------------------\n",
      "loss: 0.382522 [    0/50000]\n",
      "loss: 0.159947 [10000/50000]\n",
      "loss: 0.313358 [20000/50000]\n",
      "loss: 0.269973 [30000/50000]\n",
      "loss: 0.284715 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.0%, Avg Loss: 0.562483\n",
      "\n",
      "Epoch 80:\n",
      "------------------------------\n",
      "loss: 0.306203 [    0/50000]\n",
      "loss: 0.154044 [10000/50000]\n",
      "loss: 0.129295 [20000/50000]\n",
      "loss: 0.179858 [30000/50000]\n",
      "loss: 0.240827 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.5%, Avg Loss: 0.546637\n",
      "\n",
      "Epoch 81:\n",
      "------------------------------\n",
      "loss: 0.306906 [    0/50000]\n",
      "loss: 0.210557 [10000/50000]\n",
      "loss: 0.151556 [20000/50000]\n",
      "loss: 0.178516 [30000/50000]\n",
      "loss: 0.150152 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.4%, Avg Loss: 0.555879\n",
      "\n",
      "Epoch 82:\n",
      "------------------------------\n",
      "loss: 0.225259 [    0/50000]\n",
      "loss: 0.103940 [10000/50000]\n",
      "loss: 0.261256 [20000/50000]\n",
      "loss: 0.206021 [30000/50000]\n",
      "loss: 0.170291 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.0%, Avg Loss: 0.566822\n",
      "\n",
      "Epoch 83:\n",
      "------------------------------\n",
      "loss: 0.138324 [    0/50000]\n",
      "loss: 0.126888 [10000/50000]\n",
      "loss: 0.243951 [20000/50000]\n",
      "loss: 0.202470 [30000/50000]\n",
      "loss: 0.129026 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.9%, Avg Loss: 0.561832\n",
      "\n",
      "Epoch 84:\n",
      "------------------------------\n",
      "loss: 0.212942 [    0/50000]\n",
      "loss: 0.134261 [10000/50000]\n",
      "loss: 0.086791 [20000/50000]\n",
      "loss: 0.110753 [30000/50000]\n",
      "loss: 0.185514 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.8%, Avg Loss: 0.567243\n",
      "\n",
      "Epoch 85:\n",
      "------------------------------\n",
      "loss: 0.131054 [    0/50000]\n",
      "loss: 0.180928 [10000/50000]\n",
      "loss: 0.088106 [20000/50000]\n",
      "loss: 0.089291 [30000/50000]\n",
      "loss: 0.068252 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.2%, Avg Loss: 0.547421\n",
      "\n",
      "Epoch 86:\n",
      "------------------------------\n",
      "loss: 0.145552 [    0/50000]\n",
      "loss: 0.161649 [10000/50000]\n",
      "loss: 0.260626 [20000/50000]\n",
      "loss: 0.115405 [30000/50000]\n",
      "loss: 0.159543 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.8%, Avg Loss: 0.592102\n",
      "\n",
      "Epoch 87:\n",
      "------------------------------\n",
      "loss: 0.144449 [    0/50000]\n",
      "loss: 0.156514 [10000/50000]\n",
      "loss: 0.159951 [20000/50000]\n",
      "loss: 0.119520 [30000/50000]\n",
      "loss: 0.111963 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.7%, Avg Loss: 0.579066\n",
      "\n",
      "Epoch 88:\n",
      "------------------------------\n",
      "loss: 0.077937 [    0/50000]\n",
      "loss: 0.090443 [10000/50000]\n",
      "loss: 0.207896 [20000/50000]\n",
      "loss: 0.195219 [30000/50000]\n",
      "loss: 0.190564 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.0%, Avg Loss: 0.585254\n",
      "\n",
      "Epoch 89:\n",
      "------------------------------\n",
      "loss: 0.065712 [    0/50000]\n",
      "loss: 0.087064 [10000/50000]\n",
      "loss: 0.115086 [20000/50000]\n",
      "loss: 0.105865 [30000/50000]\n",
      "loss: 0.123062 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.0%, Avg Loss: 0.583942\n",
      "\n",
      "Epoch 90:\n",
      "------------------------------\n",
      "loss: 0.085710 [    0/50000]\n",
      "loss: 0.208012 [10000/50000]\n",
      "loss: 0.056008 [20000/50000]\n",
      "loss: 0.084241 [30000/50000]\n",
      "loss: 0.082038 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.1%, Avg Loss: 0.573804\n",
      "\n",
      "Epoch 91:\n",
      "------------------------------\n",
      "loss: 0.097335 [    0/50000]\n",
      "loss: 0.129896 [10000/50000]\n",
      "loss: 0.137617 [20000/50000]\n",
      "loss: 0.098454 [30000/50000]\n",
      "loss: 0.160391 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.5%, Avg Loss: 0.583132\n",
      "\n",
      "Epoch 92:\n",
      "------------------------------\n",
      "loss: 0.126578 [    0/50000]\n",
      "loss: 0.047944 [10000/50000]\n",
      "loss: 0.124947 [20000/50000]\n",
      "loss: 0.089954 [30000/50000]\n",
      "loss: 0.107440 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 83.7%, Avg Loss: 0.606525\n",
      "\n",
      "Epoch 93:\n",
      "------------------------------\n",
      "loss: 0.119073 [    0/50000]\n",
      "loss: 0.051266 [10000/50000]\n",
      "loss: 0.151655 [20000/50000]\n",
      "loss: 0.086084 [30000/50000]\n",
      "loss: 0.128300 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.6%, Avg Loss: 0.585219\n",
      "\n",
      "Epoch 94:\n",
      "------------------------------\n",
      "loss: 0.196937 [    0/50000]\n",
      "loss: 0.096161 [10000/50000]\n",
      "loss: 0.056937 [20000/50000]\n",
      "loss: 0.151018 [30000/50000]\n",
      "loss: 0.152168 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.5%, Avg Loss: 0.592671\n",
      "\n",
      "Epoch 95:\n",
      "------------------------------\n",
      "loss: 0.148297 [    0/50000]\n",
      "loss: 0.038985 [10000/50000]\n",
      "loss: 0.095884 [20000/50000]\n",
      "loss: 0.113040 [30000/50000]\n",
      "loss: 0.028930 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.3%, Avg Loss: 0.594981\n",
      "\n",
      "Epoch 96:\n",
      "------------------------------\n",
      "loss: 0.058532 [    0/50000]\n",
      "loss: 0.118758 [10000/50000]\n",
      "loss: 0.071399 [20000/50000]\n",
      "loss: 0.070199 [30000/50000]\n",
      "loss: 0.149631 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.5%, Avg Loss: 0.602781\n",
      "\n",
      "Epoch 97:\n",
      "------------------------------\n",
      "loss: 0.069481 [    0/50000]\n",
      "loss: 0.066375 [10000/50000]\n",
      "loss: 0.091048 [20000/50000]\n",
      "loss: 0.112887 [30000/50000]\n",
      "loss: 0.119176 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.8%, Avg Loss: 0.590726\n",
      "\n",
      "Epoch 98:\n",
      "------------------------------\n",
      "loss: 0.077439 [    0/50000]\n",
      "loss: 0.128196 [10000/50000]\n",
      "loss: 0.106807 [20000/50000]\n",
      "loss: 0.113416 [30000/50000]\n",
      "loss: 0.076205 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.4%, Avg Loss: 0.580229\n",
      "\n",
      "Epoch 99:\n",
      "------------------------------\n",
      "loss: 0.121422 [    0/50000]\n",
      "loss: 0.053399 [10000/50000]\n",
      "loss: 0.080064 [20000/50000]\n",
      "loss: 0.112381 [30000/50000]\n",
      "loss: 0.025207 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.6%, Avg Loss: 0.599689\n",
      "\n",
      "Epoch 100:\n",
      "------------------------------\n",
      "loss: 0.042049 [    0/50000]\n",
      "loss: 0.084079 [10000/50000]\n",
      "loss: 0.062442 [20000/50000]\n",
      "loss: 0.103815 [30000/50000]\n",
      "loss: 0.149826 [40000/50000]\n",
      "Test Results:\n",
      " Accuracy: 84.7%, Avg Loss: 0.592220\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "results = defaultdict(list)\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}:')\n",
    "    print('-' * 30)\n",
    "\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer, scheduler, device)\n",
    "    test_loss, test_acc = test(test_dataloader, model, loss_fn, device)\n",
    "    \n",
    "    results['train_loss'].append(train_loss)\n",
    "    results['test_loss'].append(test_loss)\n",
    "    results['test_acc'].append(test_acc)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cce7a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure, axis = plt.subplots(1, 2, figsize=(10, 5))\n",
    "epoch_nums = range(1, epochs+1)\n",
    "\n",
    "axis[0].plot(epoch_nums, results['train_loss'])\n",
    "axis[0].plot(epoch_nums, results['test_loss'])\n",
    "axis[0].set_xlabel('Epoch')\n",
    "axis[0].set_ylabel('Loss')\n",
    "axis[0].set_title('Loss')\n",
    "axis[0].legend(['train', 'test'])\n",
    "\n",
    "axis[1].plot(epoch_nums, results['test_acc'])\n",
    "axis[1].set_xlabel('Epoch')\n",
    "axis[1].set_ylabel('Accuracy')\n",
    "axis[1].set_title('Accuracy')\n",
    "axis[1].legend(['test'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5612c13",
   "metadata": {},
   "source": [
    "## 회고\n",
    "#### 처음 논문의 기술을 구현해보아서 그런지 단순히 코드를 따라치는게아니라 논문을 이해하는 능력이 필요한 것을 느꼈다. 구현단계에서 레이어를 하나하나 쌓는다는 개념이 어려웠다. 배치사이즈를 줄이니 3%가량 정확도가 올랐다. 많이 놓친부분이있는만큼 다시짚어봐야겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
